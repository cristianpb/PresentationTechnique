<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Cristian Perez">
  <title>Partage de connaissance</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Partage de connaissance</h1>
  <p class="author">Cristian Perez</p>
  <p class="date">28 février 2017</p>
</section>

<section id="anatomy-of-an-image-classifier" class="slide level2">
<h2>Anatomy of an image classifier</h2>
<figure>
<img src="images/image-classification-pipeline.jpg" style="width:100.0%" />
</figure>
<aside class="notes">
<ul>
<li>Many traditional computer vision image classification algorithms follow this pipeline</li>
<li>Deep Learning based algorithms bypass the feature extraction step completely</li>
</ul>
</aside>
</section>
<section><section id="preprocessing" class="titleslide slide level1"><h1>Preprocessing</h1></section><section id="deskew" class="slide level2">
<h2>Deskew</h2>
<p>Align an image to a reference assits the classification algorithm <a href="http://docs.opencv.org/trunk/dd/d3b/tutorial_py_svm_opencv.html">1</a>, <a href="https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/">2</a>.</p>
<figure>
<img src="images/deskew1.jpg" style="height:100.0%" />
</figure>
<aside class="notes">
<p>Il est possible d'aider l'algorithme d'apprentissage. Pour le cas de reconnaissance de visage par exemple, aligner les images par rapport par rapport à la position des yeux améliore les résultats.</p>
<p>Pour le cas de digits, aligner les numéros améliore les résultats. L'inclination de l'écriture peut être corrigé. Ainsi l'algorithme ne doit pas apprendre cette variation entre les chiffres.</p>
</aside>
</section><section class="slide level2">

<p>Deskewing simple grayscale images can be achieved using image moments (distance and intensity of pixels).</p>
<aside class="notes">
<p>This deskewing of simple grayscale images can be achieved using image moments. OpenCV has an implementation of moments and it comes in handy while calculating useful information like centroid, area, skewness of simple images with black backgrounds.</p>
<p>It turns out that a measure of the skewness is the given by the ratio of the two central moments ( mu11 / mu02 ). The skewness thus calculated can be used in calculating an affine transform that deskews the image.</p>
</aside>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> deskew(img):
    m <span class="op">=</span> cv2.moments(img)
    <span class="cf">if</span> <span class="bu">abs</span>(m[<span class="st">&#39;mu02&#39;</span>]) <span class="op">&lt;</span> <span class="fl">1e-2</span>:
        <span class="co"># no deskewing needed. </span>
        <span class="cf">return</span> img.copy()
    <span class="co"># Calculate skew based on central momemts. </span>
    skew <span class="op">=</span> m[<span class="st">&#39;mu11&#39;</span>]<span class="op">/</span>m[<span class="st">&#39;mu02&#39;</span>]
    <span class="co"># Calculate affine transform to correct skewness. </span>
    M <span class="op">=</span> np.float32([[<span class="dv">1</span>, skew, <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>SZ<span class="op">*</span>skew], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]])
    <span class="co"># Apply affine transform</span>
    img <span class="op">=</span> cv2.warpAffine(img, M, (SZ, SZ), flags<span class="op">=</span>cv2.WARP_INVERSE_MAP <span class="op">|</span> cv2.INTER_LINEAR)
    <span class="cf">return</span> img</code></pre></div>
</section><section class="slide level2">

<h3 id="not-that-easy-for-fishes">Not that easy for fishes</h3>
<figure>
<img src="images/deskewed.png" style="height:50.0%" />
</figure>
</section><section id="histogram-equalization" class="slide level2">
<h2>Histogram equalization</h2>
<p>Increase image contrast using the image's histogram.</p>
<figure>
<img src="images/histogram_equalization.png" style="width:100.0%" />
</figure>
<aside class="notes">
<ul>
<li>Brighter image will have all pixels confined to high values</li>
<li>But a good image will have pixels from all regions of the image</li>
</ul>
</aside>
</section><section class="slide level2">

<p>Palette change by a transformation function which maps the input pixels in brighter region to the output pixels in full region.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">img <span class="op">=</span> cv2.imread(<span class="st">&#39;wiki.jpg&#39;</span>,<span class="dv">0</span>)
equ <span class="op">=</span> cv2.equalizeHist(img)
res <span class="op">=</span> np.hstack((img,equ)) <span class="co">#stacking images side-by-side</span>
cv2.imwrite(<span class="st">&#39;res.png&#39;</span>,res)</code></pre></div>
<figure>
<img src="images/equalization_opencv.jpg" />
</figure>
</section><section class="slide level2">

<ul>
<li>Histogram equalization considers the global contrast of the image</li>
<li>The background contrast improves after histogram equalization, but the face of statue lost most of the information there due to over-brightness.</li>
</ul>
<p><img src="images/clahe_1.svg" style="height:30.0%" /> <a href="http://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html">1</a></p>
</section><section class="slide level2">

<h3 id="adaptive-histogram-equalization">Adaptive Histogram Equalization</h3>
<ul>
<li>Histogram is equalized inside blocks.</li>
<li>Histogram would confine to a small region (unless there is noise).</li>
<li>If noise is there, it will be amplified. To avoid this, contrast limiting is applied.</li>
<li>If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization.</li>
<li>After equalization, to remove artefacts in tile borders, bilinear interpolation is applied.</li>
</ul>
</section><section class="slide level2">

<figure>
<img src="images/clahe_2.svg" />
</figure>
</section><section class="slide level2">

<h3 id="example-using-fishes">Example using fishes</h3>
<p>Gray scale, <a href="http://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.html">doc histrogram opencv</a>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Read image</span>
src <span class="op">=</span> cv2.imread(<span class="st">&quot;img_07473.jpg&quot;</span>, cv2.IMREAD_GRAYSCALE)<span class="op">;</span>
hist <span class="op">=</span> cv2.calcHist(src,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>])
<span class="co"># Histogram equalization</span>
equ <span class="op">=</span> cv2.equalizeHist(src)
equ_hit <span class="op">=</span> cv2.calcHist(equ,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>])
<span class="co"># Create a AdaptativeHistogramEqualization object</span>
clahe <span class="op">=</span> cv2.createCLAHE(clipLimit<span class="op">=</span><span class="fl">2.0</span>, tileGridSize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))
cl1 <span class="op">=</span> clahe.<span class="bu">apply</span>(src)
cl1_hist <span class="op">=</span> cv2.calcHist(cl1,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>])</code></pre></div>
</section><section class="slide level2">

<figure>
<img src="images/hist_gray.jpg" />
</figure>
</section><section class="slide level2">

<h3 id="histogram-equalization-for-color-images">Histogram equalization for color images</h3>
<ul>
<li>Histogram equalization is a nonlinear process.</li>
<li>The concept of histogram equalization is only applicable to the intensity values in the image.</li>
<li>Convert it to a color space where intensity is separated from the color information (i.e. <a href="https://en.wikipedia.org/wiki/YUV"><strong>YUV</strong></a> color space)</li>
<li>Equalize the Y-channel and combine it with the other two channels.</li>
</ul>
<aside class="notes">
<ul>
<li>Histogram equalization is that it's a nonlinear process.</li>
<li>we cannot just separate out the three channels in an RGB image, equalize the histogram separately, and combine them later to form the output image.</li>
<li>The concept of histogram equalization is only applicable to the intensity values in the image.</li>
<li><p>So, we have to make sure not to modify the color information when we do this.</p></li>
<li>In order to handle the histogram equalization of color images, we need to convert it to a color space where intensity is separated from the color information.</li>
<li>YUV is a good example of such a color space. Once we convert it to YUV, we just need to equalize the Y-channel and combine it with the other two channels to get the output image.</li>
<li>Y′ stands for the luma component (the brightness) and U and V are the chrominance (color) components; luminance is denoted by Y and luma by Y′ – the prime symbols (') denote gamma compression.</li>
</ul>
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Read image</span>
img <span class="op">=</span> cv2.imread(<span class="st">&#39;img_07473.jpg&#39;</span>)
img_yuv <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2YUV) <span class="co"># transform to YUV</span>
hist <span class="op">=</span> cv2.calcHist(img_yuv,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>]) <span class="co"># histogram for original image</span>
<span class="co"># equalize the histogram of the Y channel</span>
img_yuv[:,:,<span class="dv">0</span>] <span class="op">=</span> cv2.equalizeHist(img_yuv[:,:,<span class="dv">0</span>])
<span class="co"># convert the YUV image back to RGB format</span>
img_output <span class="op">=</span> cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)
equ_hit <span class="op">=</span> cv2.calcHist(img_output,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>])
<span class="co"># Create a CLAHE object (Arguments are optional).</span>
clahe <span class="op">=</span> cv2.createCLAHE(clipLimit<span class="op">=</span><span class="fl">10.0</span>, tileGridSize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))
img_yuv <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
img_yuv[:,:,<span class="dv">0</span>] <span class="op">=</span> clahe.<span class="bu">apply</span>(img_yuv[:,:,<span class="dv">0</span>])
cl1 <span class="op">=</span> cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)
cl1_hist <span class="op">=</span> cv2.calcHist(img_yuv,[<span class="dv">0</span>],<span class="va">None</span>,[<span class="dv">256</span>],[<span class="dv">0</span>,<span class="dv">256</span>])</code></pre></div>
</section><section class="slide level2">

<figure>
<img src="images/hist_equalization.jpg" />
</figure>
</section><section id="image-thresholding" class="slide level2">
<h2>Image thresholding</h2>
<ul>
<li>Method of image segmentation</li>
<li>If pixel value is greater than a threshold value, it is assigned one value, else it is assigned another value.</li>
</ul>
<figure>
<img src="images/threshold.png" style="height:80.0%" />
</figure>
<aside class="notes">
<ul>
<li>Converts a gray-scale image into a binary image</li>
<li>The two levels are assigned to pixels that are below or above the specified threshold value.</li>
<li>If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black).</li>
</ul>
</aside>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Thresholding with threshold value set 127</span>
th, dst <span class="op">=</span> cv2.threshold(src,<span class="dv">127</span>,<span class="dv">255</span>, cv2.THRESH_BINARY)<span class="op">;</span>
cv2.imwrite(<span class="st">&quot;opencv-thresh-binary.jpg&quot;</span>, dst)<span class="op">;</span>

<span class="co"># Thresholding using THRESH_TOZERO</span>
th, dst <span class="op">=</span> cv2.threshold(src,<span class="dv">127</span>,<span class="dv">255</span>, cv2.THRESH_TOZERO)<span class="op">;</span>
cv2.imwrite(<span class="st">&quot;opencv-thresh-tozero.jpg&quot;</span>, dst)<span class="op">;</span>

<span class="co"># Thresholding using THRESH_TOZERO_INV</span>
th, dst <span class="op">=</span> cv2.threshold(src,<span class="dv">127</span>,<span class="dv">255</span>, cv2.THRESH_TOZERO_INV)<span class="op">;</span>
cv2.imwrite(<span class="st">&quot;opencv-thresh-to-zero-inv.jpg&quot;</span>, dst)<span class="op">;</span></code></pre></div>
</section><section id="adaptive-thresholding" class="slide level2">
<h2>Adaptive thresholding</h2>
<ul>
<li>The algorithm calculate the threshold for a small regions of the image.</li>
<li>Different thresholds for different regions of the same image</li>
<li>Gives better results for images with varying illumination.</li>
</ul>
</section><section class="slide level2">

<figure>
<img src="images/ada_threshold.jpeg" />
</figure>
</section><section id="otsus-binarization" class="slide level2">
<h2>Otsu’s Binarization</h2>
<ul>
<li>Automatically finds a threshold value which lies in between two peaks such that variances to both classes are minimum</li>
<li><a href="http://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_Image_Global_Thresholding_Adaptive_Thresholding_Otsus_Binarization_Segmentations.php">Otsu</a></li>
</ul>
<aside class="notes">
<p>But consider a bimodal image (In simple words, bimodal image is an image whose histogram has two peaks). For that image, we can approximately take a value in the middle of those peaks as threshold value, right ? That is what Otsu binarization does. So in simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.)</p>
</aside>
</section><section class="slide level2">

<figure>
<img src="images/thresholding.png" />
</figure>
</section></section>
<section><section id="feature-extraction" class="titleslide slide level1"><h1>Feature Extraction</h1></section><section id="understanding-features" class="slide level2">
<h2>Understanding features</h2>
<p>Find the exact location of these patches in the original image.</p>
<figure>
<img src="images/feature_building.jpg" />
</figure>
<aside class="notes">
<ul>
<li><p>A and B sont des surfaces plattes et sont étalés dans bcp de surfaces. Il est difficile de trouver la location exacte de ces carrés.</p></li>
<li><p>C and D sont plus simples, ils sont des bords de batiments. Vous pouvez trouver la position approximative mais pas l'emplacement exacte. L'image varie pas le long des bords mais ortogonalement aux bords. Les bords sont une meilleur caracteristiques comparés aux surfaces plats.</p></li>
<li>E and F sont des coins du batiment donc on peut les toruver facilement. Parce que aux coins à chaque fois que l'on deplace le carré, il va varier. Donc les coins sont une meilleur caracteritique.</li>
</ul>
</aside>
</section><section id="feature-definition" class="slide level2">
<h2>Feature definition</h2>
<ul>
<li>Piece of information which is relevant for solving the computational task related to a certain application.</li>
<li>Specific structures in the image such as points, edges or objects.</li>
<li>The result of a general neighborhood operation or feature detection applied to the image.</li>
<li>Concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.</li>
</ul>
<aside class="notes">
<p>This is the same sense as feature in machine learning and pattern recognition generally, though image processing has a very sophisticated collection of features.</p>
</aside>
</section><section class="slide level2">

<h3 id="feature-extractor">Feature extractor</h3>
<ul>
<li>A feature descriptor is a representation of an image that simplifies the image by extracting useful information and throwing away extraneous information.</li>
<li>A feature descriptor converts an image of size <code>width x height x 3</code> (channels) to a feature vector. (For HOG, the input image is of size <code>64 x 128 x 3</code> and the output feature vector is of length 3780)</li>
</ul>
<aside class="notes">
<p>The feature vector is not useful for the purpose of viewing the image. But, it is very useful for tasks like image recognition and object detection.</p>
</aside>
</section><section id="scale-invariant-feature-transform-sift" class="slide level2">
<h2>Scale-Invariant Feature Transform (SIFT)</h2>
<ul>
<li>Extract keypoints and compute its descriptor</li>
<li>Invariant to uniform scaling, orientation and illumination changes</li>
<li>Orientation is assigned to each keypoint to achieve invariance to image rotation</li>
<li>Descriptors are vectors of 128 values, calculated from orientation histogram over the neighbourhood, <a href="http://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html">docs.opencv</a>.</li>
</ul>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">img <span class="op">=</span> cv2.imread(<span class="st">&#39;img_00898.jpg&#39;</span>, <span class="dv">0</span>)
sift <span class="op">=</span> cv2.xfeatures2d.SIFT_create()
kp <span class="op">=</span> sift.detect(img)
img2 <span class="op">=</span> cv2.drawKeypoints(img,kp,<span class="va">None</span>,(<span class="dv">255</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">4</span>)
plt.imshow(img2)
plt.savefig(<span class="st">&quot;sift.png&quot;</span>)</code></pre></div>
<figure>
<img src="images/sift.png" />
</figure>
<div class="notes">
<p>Each keypoint is a special structure which has many attributes like its (x,y) coordinates, size of the meaningful neighbourhood, angle which specifies its orientation, response that specifies strength of keypoints etc.</p>
<ul>
<li>Orientation: A neighbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region
</div></li>
</ul>
</section><section id="speeded-up-robust-features-surf" class="slide level2">
<h2>Speeded Up Robust Features (SURF)</h2>
<ul>
<li>In 2006, it is a speeded-up version of SIFT.</li>
<li>Rely on determinant of Hessian matrix for both scale and location.</li>
</ul>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">img <span class="op">=</span> cv2.imread(<span class="st">&#39;img_07473.jpg&#39;</span>,<span class="dv">0</span>)
surf.setHessianThreshold(<span class="dv">1000</span>)
kp, des <span class="op">=</span> surf.detectAndCompute(img,<span class="va">None</span>)
img2 <span class="op">=</span> cv2.drawKeypoints(img,kp,<span class="va">None</span>,(<span class="dv">255</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">4</span>)</code></pre></div>
<figure>
<img src="images/surf.png" />
</figure>
</section><section id="histogram-of-oriented-gradients-hog" class="slide level2">
<h2>Histogram of Oriented Gradients (HOG)</h2>
<ul>
<li>The distribution of directions of gradients are used as features</li>
<li>Gradients of an image are useful because the magnitude of gradients is large around edges and corners</li>
<li>The gradient removes a lot of non-essential information (e.g. constant colored background)</li>
</ul>
</section><section class="slide level2">

<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Calculate gradient</span>
gx <span class="op">=</span> cv2.Sobel(im, cv2.CV_32F, <span class="dv">1</span>, <span class="dv">0</span>, ksize<span class="op">=</span><span class="dv">1</span>)
gy <span class="op">=</span> cv2.Sobel(im, cv2.CV_32F, <span class="dv">0</span>, <span class="dv">1</span>, ksize<span class="op">=</span><span class="dv">1</span>)
<span class="co"># Python Calculate gradient magnitude and direction ( in degrees )</span>
mag, angle <span class="op">=</span> cv2.cartToPolar(gx, gy, angleInDegrees<span class="op">=</span><span class="va">True</span>)</code></pre></div>
</section><section class="slide level2">

<figure>
<img src="images/hog.png" />
</figure>
</section></section>
<section><section id="object-detection" class="titleslide slide level1"><h1>Object detection</h1></section><section id="libraries" class="slide level2">
<h2>Libraries</h2>
<ul>
<li>Dlib <a href="http://dlib.net/ml.html#structural_object_detection_trainer">Object_detector</a></li>
<li>Opencv <a href="http://docs.opencv.org/master/d7/d8b/tutorial_py_face_detection.html">Cascade Clssfifier</a></li>
<li>Deep learning</li>
</ul>
<aside class="notes">
<p>In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) of 2012, an algorithm based on Deep Learning by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton shook the computer vision world with an astounding 85% accuracy — 11% better than the algorithm that won the second place! In ILSVRC 2012, this was the only Deep Learning based entry. In 2013, all winning entries were based on Deep Learning and in 2015 multiple Convolutional Neural Network (CNN) based algorithms surpassed the human recognition rate of 95%.</p>
<p>With such huge success in image recognition, Deep Learning based object detection was inevitable. Techniques like Faster R-CNN produce jaw-dropping results over multiple object classes. We will learn about these in later posts, but for now keep in mind that if you have not looked at Deep Learning based image recognition and object detection algorithms for your applications, you may be missing out on a huge opportunity to get better results.</p>
</aside>
</section></section>
<section><section id="conclusions" class="titleslide slide level1"><h1>Conclusions</h1></section><section class="slide level2">

<ul>
<li>Image preprocessing can significantly increase the performance of a classification algorithm.</li>
<li>A feature descriptor represents a simplified version of an image by extracting useful information and throwing away extraneous information.</li>
<li>Using feature description increases training speed compared with raw images.</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
              { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
